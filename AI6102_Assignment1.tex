\documentclass{article}
\usepackage{booktabs}
\usepackage{amsmath, amssymb, bm}
\usepackage{geometry}
\usepackage{float}    
\geometry{margin=1in}

\title{AI6102 Assignment 1}
\author{Sourabh Vyas}
\date{16/09/2025}

\begin{document}

\maketitle

%===============================================================
\section*{Question 1 (10 marks)}
Consider a multi‐class classification problem of \(C\) classes.  Based on the parametric forms of the conditional probabilities of each class introduced on the 39th Page (“Extension to Multiple Classes”) of the lecture notes of L4, derive the learning procedure of regularized logistic regression for multi‐class classification problems.  

Hint: define a loss function by borrowing an idea from binary classification, and derive the gradient descent rules to update \(\{\mathbf{w}^{(c)}\}\) for \(c=1,\dots,C-1\).

\bigskip
\subsection*{Answer}

\paragraph{1. Conditional Probabilities}
We treat class \(0\) as the baseline and model the others via a “softmax” over negative scores. Then for \(c=0,\dots,C-1\):


\[
P(y=0 \mid \mathbf{x})
=
\frac{1}
     {1 + \sum_{k=1}^{C-1}\exp\bigl(-\mathbf{w}^{(k)\,\top}\mathbf{x}\bigr)},
\quad
P(y=c \mid \mathbf{x})
=
\frac{\exp\bigl(-\mathbf{w}^{(c)\,\top}\mathbf{x}\bigr)}
     {1 + \sum_{k=1}^{C-1}\exp\bigl(-\mathbf{w}^{(k)\,\top}\mathbf{x}\bigr)}.
\]

This ensures all \(C\) probabilities sum to 1.


\paragraph{2. Multiclass Negative Log Likelihood}
We assume each example (\(x_i\),\(y_i\)) is drawn independently, so the likelihood of all labels given all inputs is
\[
L(\{\mathbf{w}^{(c)}\}) = \prod_{i=1}^N P(y_i \mid \mathbf{x}_i)
\]

Taking the logarithm turns the product into a sum:
\[
\ell(\{\mathbf{w}^{(c)}\}) = \sum_{i=1}^N \log P(y_i \mid \mathbf{x}_i)
\]

Two cases for each \(i\):

If \(y = c \;|\; c = 1, \dots, C-1\):
\[
P\bigl(y_i = c \mid \mathbf{x}_i\bigr) =
\frac{\exp\bigl(-\mathbf{w}^{(c)\top} \mathbf{x}_i\bigr)}{D_i},
\quad
D_i = 1 + \sum_{k=1}^{C-1} \exp\bigl(-\mathbf{w}^{(k)\top} \mathbf{x}_i\bigr)
\]

Taking its log
\[
\log P(y_i=c\mid \mathbf{x}_i) = -\,
\mathbf{w}^{(c)\top}\mathbf{x}_i - \log D_i
\]

If $y_i=0$:
\[
P(y_i=0\mid\mathbf{x}_i) = \frac{1}{D_i},
\quad
\log P(y_i=0\mid\mathbf{x}_i) = -\log D_i
\]


Introduce \(\mathbb{I}(y_i=c)\) which is \(1\) when \(y_i=c\). Then for each $i$
\[
-\log P(y_i\mid \mathbf{x}_i) = \log D_i + \sum_{c=1}^{C-1}\mathbb{I}(y_i=c)\,
\mathbf{w}^{(c)\top}\mathbf{x}_i
\]

Summing the negative log‐probabilities over \(i=1,\dots,N\) yields the total unregularized loss:
\[
\mathcal{L}
= \sum_{i=1}^N
  \Bigl[
    \log\bigl(1 + \sum_{k=1}^{C-1}e^{-\mathbf{w}^{(k)\top}\mathbf{x}_i}\bigr)
    + \sum_{c=1}^{C-1}\mathbb{I}(y_i=c)\,\mathbf{w}^{(c)\top}\mathbf{x}_i
  \Bigr]
\]

\paragraph{3. Regularized Negative Log‐Likelihood}


The unregularized negative log‐likelihood is


\[
\mathcal{L}
= \sum_{i=1}^N
  \Bigl[
    \log\bigl(1 + \sum_{k=1}^{C-1}e^{-\mathbf{w}^{(k)\top}\mathbf{x}_i}\bigr)
    + \sum_{c=1}^{C-1}\mathbb{I}(y_i=c)\,\mathbf{w}^{(c)\top}\mathbf{x}_i
  \Bigr]
\]


Add an \(\ell_2\) penalty:


\[
\mathcal{L}_{\mathrm{reg}}
=
\mathcal{L}
\;+\;
\frac{\lambda}{2}\sum_{c=1}^{C-1}\bigl\|\mathbf{w}^{(c)}\bigr\|^2.
\]



\paragraph{4. Gradient Computation}

Regularized negative log-likelihood for class \(c\):

\[
\mathcal{L}_{\mathrm{reg}}
=\sum_{i=1}^N
\Bigl[
\log\bigl(1 + \sum_{k=1}^{C-1}e^{-\mathbf{w}^{(k)\top}\mathbf{x}_i}\bigr)
+\sum_{k=1}^{C-1}\mathbb{I}(y_i=k)\,\mathbf{w}^{(k)\top}\mathbf{x}_i
\Bigr]
+\frac{\lambda}{2}\sum_{k=1}^{C-1}\|\mathbf{w}^{(k)}\|^2
\]


Consider the loss for a single example \(i\):
\[
\ell_i
=\log D_i
+\sum_{k=1}^{C-1}\mathbb{I}(y_i=k)\,\mathbf{w}^{(k)\top}\mathbf{x}_i,
\quad
D_i = 1 + \sum_{k=1}^{C-1}e^{-\mathbf{w}^{(k)\top}\mathbf{x}_i}
\]

Taking thr gradient w.r.t \(\mathbf{w}^{(c)}\) splits into two:
Derivative of \(\log D_i\):
\[
\frac{\partial}{\partial \mathbf{w}^{(c)}}\log D_i
=\frac{1}{D_i}\,
\frac{\partial}{\partial \mathbf{w}^{(c)}}
\bigl(1 + \sum_{k}e^{-\mathbf{w}^{(k)\top}\mathbf{x}_i}\bigr)
=-\frac{e^{-\mathbf{w}^{(c)\top}\mathbf{x}_i}}{D_i}\,\mathbf{x}_i
=-P(y=c\mid \mathbf{x}_i)\,\mathbf{x}_i
\]

Derivative of indicator term:
\[
\frac{\partial}{\partial \mathbf{w}^{(c)}}
\bigl[\mathbb{I}(y_i=c)\,\mathbf{w}^{(c)\top}\mathbf{x}_i\bigr]
=\mathbb{I}(y_i=c)\,\mathbf{x}_i
\]

Combining these two for an \(i\):
\[
\frac{\partial \ell_i}{\partial \mathbf{w}^{(c)}}
=
\bigl(\mathbb{I}(y_i=c)-P(y=c\mid \mathbf{x}_i)\bigr)\,\mathbf{x}_i
\]


Summing the per-example gradients over \(i=1,\dots,N\) yields 
\[
\nabla_{\mathbf{w}^{(c)}}\mathcal{L}
=
\sum_{i=1}^N
\bigl(\mathbb{I}(y_i=c)-P(y=c\mid \mathbf{x}_i)\bigr)\,\mathbf{x}_i
\]

Gradient for regularizer
\[
\frac{\partial (\frac{\lambda}{2}\bigl\|\mathbf{w}^{(c)}\bigr\|^2)}
{\partial \mathbf{w}^{(c)}}
= \lambda\mathbf{w}^{(c)}
\]

Adding the \(\ell_2\) Regularizer

\[
\nabla_{\mathbf{w}^{(c)}}\mathcal{L}_{\mathrm{reg}}
=
\sum_{i=1}^N
\bigl(\mathbb{I}(y_i=c)-P(y=c\mid \mathbf{x}_i)\bigr)\,\mathbf{x}_i
+\lambda\,\mathbf{w}^{(c)}
\]


\paragraph{5. Gradient Descent Update}
With learning rate \(\eta>0\), update each class‐weight:


\[
\mathbf{w}^{(c)} \;\leftarrow\;
\mathbf{w}^{(c)}
-\eta\Bigl[\nabla_{\mathbf{w}^{(c)}}\mathcal{L}_{\mathrm{reg}}\Bigr]
\]

\[
\mathbf{w}^{(c)} \;\leftarrow\;
\mathbf{w}^{(c)}
-\eta\Bigl[\sum_{i=1}^N
\bigl(\mathbb{I}(y_i=c)-P(y=c\mid \mathbf{x}_i)\bigr)\,\mathbf{x}_i
+\lambda\,\mathbf{w}^{(c)}\Bigr]
\]


\paragraph{5. Prediction Rule}
For a new input \(\mathbf{x}^*\),


\[
y^*
=
\arg\max_{c\in\{0,\dots,C-1\}}P(y=c\mid \mathbf{x}^*).
\]






\section{Question 2 (5 marks)}
This is a hands-on exercise to use the SVC API of scikit-learn1 to train a SVM with the linear kernel and the rbf kernel, respectively, on a binary classification dataset. The details of instructions are described as follows.

\bigskip
\subsection{Download the a9a dataset \(\dots\)  Detailed information is available here.}
\subsection{Regarding the linear kernel, show 3-fold cross-validation \(\dots\) training set (in accuracy).}

\begin{table}[H]
  \centering
  \caption{The 3‐fold cross‐validation results of varying values of $C$ in SVC with linear kernel on the a9a training set (in accuracy).}
  \label{tab:linear‐cv‐linear}
  \begin{tabular}{ccccc}
    \toprule
    $C=0.01$ & $C=0.05$ & $C=0.1$ & $C=0.5$ & $C=1$ \\
    \midrule
    0.8435 & 84.70\% & 84.72\% & 84.80\% & 84.83\% \\
    \bottomrule
  \end{tabular}
\end{table}

Parameters:
\begin{itemize}
  \item \texttt{StratifiedKFold}:
    \begin{itemize}
      \item \texttt{n\_splits} = 3
      \item \texttt{shuffle} = \texttt{True}
      \item \texttt{random\_state} = 64
    \end{itemize}
  \item \texttt{SVC}:
    \begin{itemize}
      \item \texttt{kernel} = \texttt{"linear"}
      \item \texttt{C} takes values in \{0.01, 0.05, 0.1, 0.5, 1\}
    \end{itemize}
  \item \texttt{cross\_val\_score}:
    \begin{itemize}
      \item \texttt{cv} = \texttt{skf} (the StratifiedKFold)
      \item \texttt{scoring} = \texttt{"accuracy"}
    \end{itemize}
\end{itemize}


\subsection{Regarding the rbf kernel, show 3-fold \(\dots\) Some examples can be found here.}

\begin{table}[H]
  \centering
  \caption{The 3‐fold cross‐validation results of varying values of $\gamma$ and $C$ in SVC with RBF kernel on the a9a training set (in accuracy).}
  \label{tab:rbf-cv}
  \begin{tabular}{lccccc}
    \toprule
    $C\!\setminus\!\gamma$ & 0.01    & 0.05    & 0.1     & 0.5     & 1      \\
    \midrule
    0.01                    & 75.92\% & 81.97\% & 81.96\% & 75.92\% & 75.92\%\\
    0.05                    & 83.04\% & 83.60\% & 83.41\% & 78.97\% & 75.92\%\\
    0.1                     & 83.74\% & 83.96\% & 83.90\% & 80.63\% & 76.10\%\\
    0.5                     & 84.33\% & 84.51\% & 84.71\% & 83.33\% & 78.95\%\\
    1                       & 84.48\% & 84.72\% & 84.76\% & 83.74\% & 79.94\%\\
    \bottomrule
  \end{tabular}
\end{table}


\subsection{Based on the results shown in Tables \(\dots\) the following table:}
\begin{table}[ht]
  \centering
  \caption{Test results of SVC on the a9a test set (in accuracy).}
  \label{tab:test-results}
  \begin{tabular}{l l c}
    \toprule
    Kernel & Parameter setting      & Test accuracy \\
    \midrule
    RBF    & $C=1,\ \gamma=0.1$     &  85.03\%  \\
    \bottomrule
  \end{tabular}
\end{table}


\section*{Question 3 (5 marks)}
The optimization problem of linear soft-margin SVMs can be re-formulated as an instance of empirical structural risk minimization (refer to Page 37 on L5 notes). Show how to reformulate it. Hint: search reference about the hinge loss.
\bigskip
\subsection*{Answer}
\paragraph{1. The standard soft-margin SVM solves}

\[
\min_{w,b,\{\xi_i\}} \;\frac{1}{2}\lVert w\rVert_2^2 \;+\; C\sum_{i=1}^N \xi_i
\quad\text{subject to}\quad
y_i\bigl(w\cdot x_i + b\bigr)\ge 1 - \xi_i,\;\xi_i\ge0
\]

\paragraph{2. Introduce the hinge loss}

\[
\ell_{\mathrm{hinge}}\bigl(f(x_i),y_i\bigr)
=\max\bigl(0,\;1 - y_i\,f(x_i)\bigr),
\]

where \(f(x) = wx + b\)

At optimum \(\xi_i = \max\bigl(0,\;1-y_i\,f(x_i)\bigr)\) Substituting back gives the unconstrained form

\[
\min_{w,b}\;\frac{1}{2}\lVert w\rVert_2^2 \;+\; C\sum_{i=1}^N \max\bigl(0,\;1 - y_i(w\cdot x_i + b)\bigr)
\]


\paragraph{3. Empirical Risk Minimization}
The empirical risk minimization template is


\[
\hat\theta
=\arg\min_{\theta}\;\sum_{i=1}^N \ell\bigl(f(x_i;\theta),y_i\bigr)
\;+\;\lambda\,\Omega(\theta).
\]



\[\text{Set }
\theta=(w,b),\quad
\ell(f(x),y)=\max\bigl(0,\,1 - y\,f(x)\bigr),\quad
\Omega(\theta)=\tfrac12\lVert w\rVert_2^2,\quad
\lambda=\tfrac1C.
\]



\section*{Question 4 (5 marks)}
Using the kernel trick introduced in L5 to extend the regularized linear regression model (L3) to solve nonlinear regression problems. Derive a closed-form solution (i.e., to derive a kernelized version of the closed-form solution on Page 50 of L3).
\bigskip
\subsection*{Answer}
\paragraph{1. The primal objective is}
\[
\min_{w\in\mathbb{R}^d}
\;J(w)
\;=\;\|Xw - y\|_2^2 \;+\;\lambda\,\|w\|_2^2.
\]

By the representer theorem, the minimizer \(w*\) can be expressed in the span of the rows of \(X\). Introduce a coefficient vector \(\alpha \in \mathbb{R}^N\) :

\[
w = X^T \alpha,
\qquad
\alpha\in\mathbb{R}^N.
\]


\paragraph{2. Objective in Terms of \(alpha\) and Differentiation}

Define the kernel (Gram) matrix \(K = X X^{T} \in \mathbb{R}^{N \times N}\).
Substitute \(w=X^{T} \alpha \) into \(J(w)\):

\[
J(\alpha) 
= \| X (X^{T} \alpha) - y \|_2^2 
+ \lambda \, \| X^{T} \alpha \|_2^2
= \| K \alpha - y \|_2^2 
+ \lambda \, \alpha^{T} (X X^{T}) \alpha
\]
\[
= \alpha^{T} K^2 \alpha 
- 2 \, y^{T} K \alpha 
+ y^{T} y 
+ \lambda \, \alpha^{T} K \alpha.
\]

Differentiate \(J(\alpha)\) wrt \(\alpha\):


\[
\begin{aligned}
\nabla_\alpha J(\alpha)
&= 2\,K\,(K\alpha - y) \;+\;2\,\lambda\,K\,\alpha \\
&= 2\,K\bigl(K\alpha - y + \lambda\,\alpha\bigr).
\end{aligned}
\]


Set \(\nabla_\alpha J(\alpha) = 0\)

\[
K\bigl(K\alpha - y + \lambda\,\alpha\bigr)=0
\;\Longrightarrow\;
(K + \lambda I)\,\alpha = y.
\]

\paragraph{3. Solving for \(\alpha\) yields the dual(kernel) closed form}

\[
\alpha 
= (K + \lambda I)^{-1}\,y.
\]

Substitute back to get the primal weight vector:


\[
w 
= X^T \alpha
= X^T (K + \lambda I)^{-1}\,y
= X^T (X X^T + \lambda I)^{-1}\,y.
\]





\end{document}
